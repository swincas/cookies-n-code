{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line-profiler\n",
    "\n",
    "This tutorial covers the basics of the [line_profiler](https://github.com/rkern/line_profiler) Python package.  I'll cover what exactly line-profiler is (and why you should care) and how to use it.\n",
    "\n",
    "\n",
    "## What is line-profiler\n",
    "\n",
    "As the name suggests line-profiler profiles the runtime of a Python script in a line-by-line manner. From the profiling you can identify **hotspots**; i.e., what lines of code take the longest time to run! While sometimes this may be of simple academic interest, alternative algorithms could be used and lead to significant time savings.  If a loop is executed a million times and you save 10ms on each loop, congratulations your code now finishes three hours sooner! \n",
    "\n",
    "## Installing line-profiler\n",
    "\n",
    "Provided you have a working Python distribution, installing should be as easy as \n",
    "```\n",
    "$ pip install line_profiler\n",
    "```\n",
    "\n",
    "If this fails, you can also clone the Github repository:\n",
    "\n",
    "```\n",
    "$ git clone https://github.com/rkern/line_profiler.git\n",
    "```\n",
    "\n",
    "And then install using \n",
    "\n",
    "```\n",
    "$ python setup.py install\n",
    "```\n",
    "\n",
    "remembering you may need the ```--user``` or ```--prefix=/directory/to/local/python/site-packages/``` argument if you're on a cluster such as ```g2```.\n",
    "\n",
    "\n",
    "## Using line-profiler with Jupyter Notebook\n",
    "\n",
    "When using line-profiler there is a slight twist depending on whether you're running the Python script directly from the terminal (e.g., ``python my_script.py``) or from inside a Jupyter notebook.  I'll go over both of these methods before showing some examples from my own code (which is all executed via command line).\n",
    "\n",
    "Loading line-profiler into your notebook is one easy command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need some functions to test our run-time on.  \n",
    "\n",
    "For the first example, say we are given particle positions ``x``, ``y`` and ``z`` and we wish to calculate the pair-wise distance between each particle. The pair-wise distance is a very useful property which allows us to compute further statistics such as the correlation function or power spectrum, and is necessary for things such as force calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.39347777  0.82736576  0.15859656]\n",
      "0.393477774111\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function # Always do this >:( \n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "pos = np.random.random((1000,3)) # Generate a 3D vector of random points between 0 and 1.\n",
    "print(pos[0])\n",
    "print(pos[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll do my favourite method; the crass, brute force method that involves nested ``for`` loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_brute(pos):\n",
    "    npart = pos.shape[0]\n",
    "    ndim = pos.shape[1] \n",
    "    distance = np.empty((npart, npart), dtype = np.float64)\n",
    "    \n",
    "    # Note that the distance between pair i-j is the same as j-i.\n",
    "    for i in range(npart): \n",
    "        for j in range(npart):\n",
    "            d = 0.0\n",
    "            for k in range(ndim):\n",
    "                d += pow(pos[i,k] - pos[j,k], 2) # Gets the (square) distance in one dimension between particles i and j.\n",
    "            distance[i,j] = np.sqrt(d)\n",
    "        assert(distance[i,i] == 0) # A particle should have zero distance from itself.\n",
    "    return distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.212877826851\n"
     ]
    }
   ],
   "source": [
    "distance = pairwise_brute(pos)\n",
    "print(distance[0,100]) # This is the distance between particle 0 and 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run ```line-profiler``` on our code.  The argument ``-f`` specifies the function that we want to profile and we need to remember to provide the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f pairwise_brute pairwise_brute(pos)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 9.24807 s\n",
    "File: <ipython-input-37-70d496d08d07>\n",
    "Function: pairwise_brute at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def pairwise_brute(pos):\n",
    "     2         1            3      3.0      0.0      npart = pos.shape[0]\n",
    "     3         1            1      1.0      0.0      ndim = pos.shape[1] \n",
    "     4         1          677    677.0      0.0      distance = np.empty((npart, npart), dtype = np.float64)\n",
    "     5                                               \n",
    "     6                                               # Note that the distance between pair i-j is the same as j-i.\n",
    "     7      1001          722      0.7      0.0      for i in range(npart): \n",
    "     8   1001000       480533      0.5      5.2          for j in range(npart):\n",
    "     9   1000000       415336      0.4      4.5              d = 0.0\n",
    "    10   4000000      2215483      0.6     24.0              for k in range(ndim):\n",
    "    11   3000000      4031247      1.3     43.6                  d += pow(pos[i,k] - pos[j,k], 2) # Gets the (square) distance in one dimension between particles i and j.\n",
    "    12   1000000      2102190      2.1     22.7              distance[i,j] = np.sqrt(d)\n",
    "    13      1000         1875      1.9      0.0          assert(distance[i,i] == 0) # A particle should have zero distance from itself.\n",
    "    14         1            1      1.0      0.0      return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful that in the above table the **Time** column is expressed in units of $10^{-6}s$, i.e., $\\mu s$.  The **Time** column tells us how many $\\mu s$ we spend on each line (total), the **Per Hit** column tells us the average amount of time spent on relative to the total amount of time spent in the function and finally the **% Time** column is the percentage of time spent on that line (relative to the total amount of time).\n",
    "\n",
    "From this table a number of things become apparent.  Firstly, I enjoy writing long comments.  Secondly, one of the most expensive line of code is line 13 where we perform our ``assert`` error check.  This really highlights the power of a tool like line-profiler because whilst this line is expensive (taking $1.9\\mu s$ per hit), the **relative** amount of time spent on this operation is small. The honour of most expensive operation goes to line 11 in which we spend $44\\%$(!!) of our time on.  This is a result of our nested ``for`` loops; we are hitting line 11 a grand total of $4031247$ times.  \n",
    "\n",
    "Let's see if we can do better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_numpy(pos):\n",
    "    padded_pos = pos[:, None, :] # Pad the array to ensure the dimensionality will be correct \n",
    "    distance = np.sqrt(np.sum(np.square(padded_pos - pos), axis = -1)) # Perform Pythag and then sum to return to the original dimensions.\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f pairwise_numpy pairwise_numpy(pos)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 0.050307 s\n",
    "File: <ipython-input-77-fb2fce4afd83>\n",
    "Function: pairwise_numpy at line 1\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     1                                           def pairwise_numpy(pos):\n",
    "     2         1            8      8.0      0.0      padded_pos = pos[:, None, :] # Pad the array to ensure the dimensionality will be correct \n",
    "     3         1        50297  50297.0    100.0      distance = np.sqrt(np.sum(np.square(padded_pos - pos), axis = -1)) # Perform Pythag and then sum to return to the original dimensions.\n",
    "     4         1            2      2.0      0.0      return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah we have remarkable speed-up relative to the brute force method.  \n",
    "**Beware:** When using Numpy broadcasting, temporary arrays are created potentially causing memory issues for (very) large arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using line-profiler with terminal\n",
    "\n",
    "If you're like me and behind the times, you can also run line-profiler from the terminal.  For this all you do is add the decorator ``@profile`` before the function you want to profile.  Then you profile the results by \n",
    "\n",
    "```\n",
    "$ kernprof -l my_script.py <Script Arguments>\n",
    "```\n",
    "\n",
    "If you get the error '``bash: kernprof: command not found``' then fully specify the path to where you installed ``line_profiler``:\n",
    "\n",
    "```\n",
    "$ python3 /home/jseiler/.local/lib/python3.5/site-packages/kernprof.py -l rewrite_files.py 15 15\n",
    "```\n",
    "\n",
    "Once the profiling has completed you can view the results via \n",
    "\n",
    "```\n",
    "$ python3 -m line_profiler my_script.py.lprof\n",
    "```\n",
    "\n",
    "As an example from my own code (I've only included the lines with $>0.0$ in the **% Time** column):\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 2114.36 s\n",
    "File: rewrite_files.py\n",
    "Function: link_fof_snapshot_full at line 322\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "   386       256      4468354  17454.5      0.2          with h5py.File(fname, 'r') as f:\n",
    "   390       256      6629947  25898.2      0.3              BoxSize = f['Header'].attrs['BoxSize'] # Size of the simulation box (in Mpc/h).\n",
    "   391       256        38943    152.1      0.0              Time = f['Header'].attrs['Time'] # Scale factor.\n",
    "   392       256        37268    145.6      0.0              Omega_m = f['Header'].attrs['Omega0'] # Matter density.\n",
    "   393       256        36343    142.0      0.0              Omega_l = f['Header'].attrs['OmegaLambda'] # Dark energy density.\n",
    "   394       256        36325    141.9      0.0              particle_mass = f['Header'].attrs['MassTable'] # Table of particle masses (length equal to TypeMax)\n",
    "   406       506      6359047  12567.3      0.3                              snapshot_partid = particles['ParticleIDs']\n",
    "   407       506   1140906728 2254756.4     54.0                              ids_found = (np.nonzero(np.in1d(snapshot_partid, fof_partid[type_idx], assume_unique = True)))[0] # np.in1d returns a True if the snapshot particle is found in the FoF list (False otherwise).  np.nonzero then returns the indices of those values that have a 'True'.\n",
    "   424       228     16848850  73898.5      0.8                                  particle_position[type_idx].append(particles['Coordinates'][ids_found]) # For some super weird reason this needs 3 indices to properly reference.  I.e. particle_position[1][0] will return all the coordinates for Particle Type 1.\n",
    "   425       228     17104189  75018.4      0.8                                  particle_velocity[type_idx].append(particles['Velocities'][ids_found])\n",
    "   426       228      1977059   8671.3      0.1                                  particle_id[type_idx].append(particles['ParticleIDs'][ids_found])\n",
    "   429       228    915102216 4013606.2     43.3                                  fof_partid_idx = np.int32(np.nonzero(np.in1d(fof_partid[type_idx], snapshot_partid, assume_unique = True))[0])\n",
    "   473       256      1306722   5104.4      0.1              print(\"Written Data to {0}.\".format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From this we can see that a massive $97.3\\%$ of time is spent doing two ```np.nonzero``` calculations.  This really puts things into perspective: if I wanted to optimize my code these are **the** lines I would start with.  It isn't even worth thinking about optimizing the other parts of my code when they form such a neglible percentage of time spent.\n",
    "\n",
    "## Wrapping Up\n",
    "\n",
    "Hopefully this tutorial has given a basic demonstration how to use ```line_profiler```. More importantly, I hope it's outlined the use of such a tool; namely identifying hotspots within your code so you know **where** to start optimization.  Blindly going through every line in your code and asking \"Could I make this faster?\" is a painful exercise and you often won't see a noticable speedup in your code. \n",
    "\n",
    "As always, the [Github page](https://github.com/rkern/line_profiler) for the package will contain a wealth of information on running the code and some tips and tricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
